---
title: "Statistical Learning - Data preparation & Supervised Learning"
author: "Angelina Khatiwada, Rijin Baby"
date: "09/06/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Set Information:

This data was collected via a survey on Amazon Mechanical Turk. The survey describes different driving scenarios including the destination, current time, weather, passenger, etc., and then ask the person whether he will accept the coupon if he is the driver.

- Multivariate dataset
- Number of Instances: 12684
- Number of Attributes: 23
- Associated Tasks: Classification
- Missing Values: Yes


## Some Attribute Information:

- expiration: 1d, 2h (the coupon expires in 1 day or in 2 hours)
- Bar: never, less1, 1~3, gt8, nan4~8 (feature meaning: how many times do you go to a bar every month?)
- CoffeeHouse: never, less1, 4~8, 1~3, gt8, nan (feature meaning: how many times do you go to a coffeehouse every month?)
- CarryAway:n4~8, 1~3, gt8, less1, never (feature meaning: how many times do you get take-away food every month?)
- RestaurantLessThan20: 4~8, 1~3, less1, gt8, never (feature meaning: how many times do you go to a restaurant with an average expense per person of less than $20 every month?)
- Restaurant20To50: 1~3, less1, never, gt8, 4~8, nan (feature meaning: how many times do you go to a restaurant with average expense per person of $20 - $50 every month?)
- toCoupon_GEQ15min:0,1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 15 minutes)
- toCoupon_GEQ25min:0, 1 (feature meaning: driving distance to the restaurant/bar for using the coupon is greater than 25 minutes)
- direction_same:0, 1 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)
- direction_opp:1, 0 (feature meaning: whether the restaurant/bar is in the same direction as your current destination)
- Y:1, 0 (whether the coupon is accepted)

```{r message=FALSE, warning=FALSE}
    library(skimr)
    library(readr)
    library(plyr)
    library(dplyr)
    library(purrr)
    library(VIM)
    library(ggplot2)
    library(plotly)
    library(caret)
    library(grid)
    library(gridExtra)
    library("epitools")
    library(pROC)
    library(MASS)
    library(class)
    library(gmodels)

```

## Dataset
```{r message=FALSE, warning=FALSE}
  in_vehicle_coupon_recommendation <- read_csv("https://raw.githubusercontent.com/rijinbaby/Statistical-Learning/main/in-vehicle-coupon-recommendation.csv")
  coupon_data <- in_vehicle_coupon_recommendation
  coupon_data <- as.data.frame(coupon_data)
  
  library(purrr)
  # View(coupon_data %>% map(table))
  coupon_data %>% map(table)
```
### Missing & unique value check
```{r}
  (colMeans(is.na(coupon_data))*100)
  coupon_data$car <- NULL # no data at all - 4 other with <2% missing
  
  which(apply(coupon_data, 2, function(x) length(unique(x)))==1)
  coupon_data$toCoupon_GEQ5min <- NULL # removing column with single value
```
```{r}
coupon_accepted <- as.factor(coupon_data$Y)

#age
t1 <- ggplot(coupon_data, aes(x=age, fill=coupon_accepted)) +
    geom_bar(stat="count")

#income 
t2 <- ggplot(coupon_data, aes(x=income, fill=coupon_accepted)) +
  geom_bar(stat="count")

#expiration
t3 <- ggplot(coupon_data, aes(x=expiration, fill=coupon_accepted)) +
  geom_bar(stat="count")

grid.arrange(t1, t2, t3, ncol=1)

```


### New Variables
```{r message=FALSE, warning=FALSE}
{
  coupon_data[] <- lapply(coupon_data, as.character)
  coupon_data$Y <- as.numeric(coupon_data$Y)
  
  # age column - Creating a new column to give numerical weightage
  table(coupon_data$age)
  coupon_data$age_weightage <- NA
  coupon_data$age_weightage[which(coupon_data$age=="below21")] <- 1
  coupon_data$age_weightage[which(coupon_data$age=="21")] <- 2
  coupon_data$age_weightage[which(coupon_data$age=="26")] <- 3
  coupon_data$age_weightage[which(coupon_data$age=="31")] <- 4
  coupon_data$age_weightage[which(coupon_data$age=="36")] <- 5
  coupon_data$age_weightage[which(coupon_data$age=="41")] <- 6
  coupon_data$age_weightage[which(coupon_data$age=="46")] <- 7
  coupon_data$age_weightage[which(coupon_data$age=="50plus")] <- 8
  table(coupon_data$age_weightage)
  
  # temp & weather
  # View(table(coupon_data$weather,coupon_data$temperature))
  
  # Income - Creating a new column to give numerical weightage
  table(coupon_data$income)
  coupon_data$income_weightage <- NA
  coupon_data$income_weightage[which(coupon_data$income=="Less than $12500")] <- 1
  coupon_data$income_weightage[which(coupon_data$income=="$12500 - $24999")] <- 2
  coupon_data$income_weightage[which(coupon_data$income=="$25000 - $37499")] <- 3
  coupon_data$income_weightage[which(coupon_data$income=="$37500 - $49999")] <- 4
  coupon_data$income_weightage[which(coupon_data$income=="$50000 - $62499")] <- 5
  coupon_data$income_weightage[which(coupon_data$income=="$62500 - $74999")] <- 6
  coupon_data$income_weightage[which(coupon_data$income=="$75000 - $87499")] <- 7
  coupon_data$income_weightage[which(coupon_data$income=="$87500 - $99999")] <- 8
  coupon_data$income_weightage[which(coupon_data$income=="$100000 or More")] <- 9
  table(coupon_data$income_weightage)
  
  # Income - Creating a new column to re-classify reference - https://en.wikipedia.org/wiki/International_Standard_Classification_of_Occupations
  (table(coupon_data$occupation))
  coupon_data$occupation_class <- NA
  coupon_data$occupation_class[which(coupon_data$occupation %in% 
                                       c("Architecture & Engineering","Arts Design Entertainment Sports & Media"
                                         ,"Business & Financial","Computer & Mathematical","Education&Training&Library"
                                         ,"Healthcare Practitioners & Technical","Legal","Management"))] <- "Professionals"
  coupon_data$occupation_class[which(coupon_data$occupation %in% 
                                       c("Building & Grounds Cleaning & Maintenance","Food Preparation & Serving Related"
                                         ,"Installation Maintenance & Repair","Transportation & Material Moving"))]  <- "Craft and related trades workers"
  coupon_data$occupation_class[which(coupon_data$occupation %in% 
                                       c("Community & Social Services","Construction & Extraction","Healthcare Support"
                                         ,"Life Physical Social Science"))] <- "Technicians and pro"
  coupon_data$occupation_class[which(coupon_data$occupation %in% 
                                       c("Personal Care & Service","Protective Service","Sales & Related"))] <- "Service and sales"
  coupon_data$occupation_class[which(coupon_data$occupation %in% 
                                       c("Farming Fishing & Forestry","Office & Administrative Support"
                                         ,"Production Occupations"))] <- "Others"  #own classification
  coupon_data$occupation_class[which(coupon_data$occupation=="Retired")] <- 'Retired' 
  coupon_data$occupation_class[which(coupon_data$occupation=="Student")] <- "Student"
  coupon_data$occupation_class[which(coupon_data$occupation=="Unemployed")] <- "Unemployed"
  
  occup_class <- coupon_data %>%
  group_by(occupation_class) %>%
  summarise("Actual_occupation" = occupation)
  occup_class <- unique(occup_class)
 
                                       
  # TIME VARIABLE
  table(coupon_data$expiration)
  coupon_data$expiration_weightage <- NA
  coupon_data$expiration_weightage[which(coupon_data$expiration=="2h")] <- 2
  coupon_data$expiration_weightage[which(coupon_data$expiration=="1d")] <- 24
  
  # passenger
  coupon_data$passanger[which(coupon_data$passanger=="Friend(s)")] <- "Friends"
  coupon_data$passanger[which(coupon_data$passanger=="Kid(s)")] <- "Kids"
  
  # education
  coupon_data$education[which(coupon_data$education=="Graduate degree (Masters or Doctorate)")] <- "Graduate degree"
  # print(table(coupon_data$occupation_class))
  
}
```
```{r}
 
knitr::kable(occup_class, format = "html")
```

### missing imputation knn approach
```{r}
  library(VIM)
  # colMeans(is.na(coupon_data))*100
  # which(colMeans(is.na(coupon_data))>0)
  cleaned_data <- kNN(coupon_data
                           , variable = c("Bar","CoffeeHouse","CarryAway","RestaurantLessThan20","Restaurant20To50")
                           , k = 5)
  cleaned_data <- cleaned_data[,1:ncol(coupon_data)]
  # coupon_data_final %>% map(table)
  colMeans(is.na(cleaned_data))*100
```
### Plotting data


```{r message=FALSE, warning=FALSE}
library(gridExtra)
cleaned_data$Y <- as.factor(cleaned_data$Y)

#Destination
p1 <- ggplot(cleaned_data, aes(x=destination, fill=Y)) +
    geom_bar(stat="count")

#passanger 
p2 <- ggplot(cleaned_data, aes(x=passanger, fill=Y)) +
  geom_bar(stat="count")

#weather
p3 <- ggplot(cleaned_data, aes(x=weather, fill=Y)) +
  geom_bar(stat="count")

#time
p4 <- ggplot(cleaned_data, aes(x=time, fill=Y)) +
  geom_bar(stat="count")

#gender
p5 <- ggplot(cleaned_data, aes(x=gender, fill=Y)) +
  geom_bar(stat="count")

#maritalStatus   
p6 <- ggplot(cleaned_data, aes(x=maritalStatus, fill=Y)) +
  geom_bar(stat="count")+
  theme(axis.text.x = element_text(angle = 15, hjust = 1))

grid.arrange(p1, p2, p3, p4, p5, p6, ncol=2)

```

```{r}
#education                       
p7 <- ggplot(cleaned_data, aes(x=education, fill=Y)) +
  geom_bar(stat="count")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p7

#occupation                                          
p8 <- ggplot(cleaned_data, aes(x=occupation_class, fill=Y)) +
  geom_bar(stat="count")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
p8

```

```{r}
#Bar                                                 
p9 <- ggplot(cleaned_data, aes(x=Bar, fill=Y)) +
  geom_bar(stat="count")

#CoffeeHouse                                       
p10 <- ggplot(cleaned_data, aes(x=CoffeeHouse, fill=Y)) +
  geom_bar(stat="count")

#CarryAway                       
p11 <- ggplot(cleaned_data, aes(x=CarryAway, fill=Y)) +
  geom_bar(stat="count")

#RestaurantLessThan20                                
p12 <- ggplot(cleaned_data, aes(x=RestaurantLessThan20, fill=Y)) +
  geom_bar(stat="count")

#direction_same                                               
p13 <- ggplot(cleaned_data, aes(x=direction_same, fill=Y)) +
  geom_bar(stat="count")

#has_children                                                
p14 <- ggplot(cleaned_data, aes(x=has_children, fill=Y)) +
  geom_bar(stat="count")

grid.arrange(p9, p10, p11, p12, p13, p14, ncol=2)

```


**For numeric variables**
```{r}
#Age histogram
p15 <- ggplot(data = cleaned_data, aes(age_weightage, color = Y))+
  geom_freqpoly(binwidth = 5, size = 1)

#Income histogram
p16 <- ggplot(data = cleaned_data, aes(income_weightage, color = Y))+
  geom_freqpoly(binwidth = 5, size = 1)

#Expiration histogram
p17 <- ggplot(data = cleaned_data, aes(expiration_weightage, color = Y))+
  geom_freqpoly(binwidth = 5, size = 1)

grid.arrange(p15, p16, p17, ncol=2)
```

## Modeling

We drop the variable 'direction_opp' that is perfectly correlated with the varible 'direction_same'.We also drop the categorical varibles 'age', 'income', 'expiration', for which we created corresponding numeric variables. 'occupation' will be replaced by 'occupation_class' in the model.  

- Y - factor
- 'age', 'income', 'expiration' - numeric
- other variables' type is characteric, so the dummies will be created automatically, removing the first dummy variable created from each column. This is done to avoid multicollinearity. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
drops= c("direction_opp", 'age', 'income', 'expiration', 'occupation') #drop additional correlated variables
cleaned_data <- cleaned_data[ , !(colnames(cleaned_data) %in% drops)]

str(cleaned_data)
```

## Logistic Regression

**Anova Chi-square test** to check the overall effect of variables on the dependent variable

**Multicollinearity check**

Dataset without splitting

```{r}

log_model <- glm(Y ~., data = cleaned_data, family=binomial(link='logit'))
summary(log_model)

anova(log_model, test = 'Chisq')

#car::vif(log_model)
```

Insignificant coefficients (with p-value > 0.05) are:

- has_children
- toCoupon_GEQ25min ??
- income_weightage
- age_weightage ??
- time ??

**Running Cross-Validation with k = 10 folds** on the training set without removing the variables

```{r message=FALSE, warning=FALSE}

#train/test split

set.seed(123)
split_train_test  <- createDataPartition(cleaned_data$Y, p = .67,
                                  list = FALSE,
                                  times = 1)
 
train <- cleaned_data[ split_train_test,]
test  <- cleaned_data[-split_train_test,]

# CV  with 10 folds
train_control <- trainControl(method = "cv", number = 10)

# train the model on training set
log_reg <- train(Y ~.,
               data = train,
               trControl = train_control,
               method = "glm",
               family=binomial(link='logit'))

log_reg

```

**Removing the variables**

```{r}
#remove variables
  cleaned_data$has_children <- NULL
  cleaned_data$toCoupon_GEQ25min <- NULL
  cleaned_data$income_weightage <- NULL

  cleaned_data$time <- NULL # perfect collinearity with predictor variable
```

```{r}
log_model <- glm(Y ~., data = cleaned_data, family=binomial(link='logit'))
summary(log_model)

anova(log_model, test = 'Chisq')

car::vif(log_model)
```

**Running Cross-Validation with k = 10 folds** on the training set with some variables removed

```{r message=FALSE, warning=FALSE}
set.seed(123)
split_train_test  <- createDataPartition(cleaned_data$Y, p = .67,
                                  list = FALSE,
                                  times = 1)
 
train <- cleaned_data[ split_train_test,]
test  <- cleaned_data[-split_train_test,]

#CV  with 10 folds
train_control <- trainControl(method = "cv", number = 10)

# train the model on training set
log_reg <- train(Y ~.,
               data = train,
               trControl = train_control,
               method = "glm",
               family=binomial(link='logit'))

log_reg
```

### Predictions

**Model accuracy and Confusion matrix**
```{r}
log_reg_prob1 <- predict(log_reg, test, type = 'prob')
log_reg_pred1 <- ifelse(log_reg_prob1[2] > 0.5, 1, 0)
mean(log_reg_pred1 == test$Y)
```

```{r}
confusionMatrix(
  as.factor(log_reg_pred1),
  as.factor(test$Y),
  positive = "1" 
)
```

High sensitivity: fewer False Negative errors. Low specificity: Many False Positive.

We can change the threshold 0.5 and set a higher threshold to balance these values.

```{r}
log_reg_prob1 <- predict(log_reg, test, type = 'prob')
log_reg_pred1 <- ifelse(log_reg_prob1[2] > 0.55, 1, 0)

confusionMatrix(
  as.factor(log_reg_pred1),
  as.factor(test$Y),
  positive = "1" 
)
```

**ROC curve**
```{r}
test_roc = roc(test$Y ~ log_reg_prob1$"1", plot = TRUE, print.auc = TRUE)
```

## Linear Discriminant Analysis

With the removed variables

```{r}
lda.fit=lda(Y~.,data = train)
lda.fit

plot(lda.fit)

```

```{r}
lda.pred=predict(lda.fit,test)$class
table(lda.pred,test$Y)
mean(lda.pred==test$Y)
```
LDA predicts more False Positives as well

## Association Rules

We analyse association between Y and the 'direction_same', which is the significant variable

```{r}
assoc <- xtabs(~Y+direction_same, data=cleaned_data)
assoc
plot(assoc, col=c("green","blue"))
```

**Testing significance**
```{r}
Test <- chisq.test(assoc, correct=FALSE)
Test
```

Chi2 is 2.7, relation between the variables is not significant (p-value> 0.05). We accept H0 about independence between the variables, there is no association.

```{r}
riskratio.wald(table(cleaned_data$direction_same,cleaned_data$Y))
oddsratio.wald(table(cleaned_data$direction_same,cleaned_data$Y))

```
Confidence interval includes 1 - accept H0 about independence. 

Odds: odds under direction same/ odds under direction opp = 1.07

```{r}
lr_fit <- glm(Y ~ direction_same, data = cleaned_data,
              family=binomial(link='logit'))
summary(lr_fit)

exp(cbind(OR = coef(lr_fit), confint(lr_fit)))

```
Direction is not a factor.

## KNN model

Uploading a dataset with dummies that are all numeric 

Algorithms which use distance based methods require all variables to be numeric
```{r}

coupon_data_encoded  <- read.csv("https://raw.githubusercontent.com/rijinbaby/Statistical-Learning/main/cleaned_data_encoded.csv")

```

```{r}
##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
coupon_data_encoded[,c("age_weightage","expiration_weightage","income_weightage")] <-
  as.data.frame(lapply(coupon_data_encoded[,c("age_weightage","expiration_weightage","income_weightage")],nor))
```


```{r}
#train/test split

set.seed(123)
split_train_test  <- createDataPartition(coupon_data_encoded$Y, p = .67,
                                  list = FALSE,
                                  times = 1)

train <- coupon_data_encoded[ split_train_test,]
test  <- coupon_data_encoded[-split_train_test,]

train_label <- train$Y
test_label <- test$Y

test$Y <- NULL
train$Y <- NULL


```

**KNN confusion matrix and accuracy for K=10**

```{r}

##run knn function

model_knn <- knn(train = train, test = test ,cl=train_label,k=10)

##create confusion matrix
tab <- table(model_knn,test_label)
tab
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)

CrossTable(x=model_knn, y=test_label, prop.chisq=FALSE) 
```

Check training and test errors for different values of K

```{r}
#loop for different  0<K<21
k <- 1
accuracy_scores_test <- c()
accuracy_scores_train <- c()
k_values <- c()
while (k < 16) {
  k_values <- c(k_values, k)
  test_knn <- knn(train = train, test = test ,cl=train_label,k=k)
  train_knn <- knn(train = train, test = train ,cl=train_label,k=k)
  tab_test <- table(test_knn,test_label)
  tab_train <- table(train_knn,train_label)
  accuracy_scores_test <- c(accuracy_scores_test, accuracy(tab_test))
  accuracy_scores_train <- c(accuracy_scores_train, accuracy(tab_train))
  k = k+1
}

acc = cbind(data.frame(k_values), data.frame(accuracy_scores_test), data.frame(accuracy_scores_train))
acc

```

Plotting accuracy for training (blue) and test sets (red)

```{r}

ggplot(acc, aes(x=k_values)) + 
  geom_line(aes(y = accuracy_scores_test), color = "red", size=1.3) + 
  geom_line(aes(y = accuracy_scores_train), color="blue", size=1.3) 

```

<!-- ## one-hot encoding -->
```{r}
  # cleaned_data$age <- NULL; cleaned_data$income <- NULL; cleaned_data$occupation<- NULL; cleaned_data$expiration <- NULL
  # 
  # # library(caret)
  # # dummy <- dummyVars(" ~ .", data=cleaned_data)
  # # coupon_data_encoded <- data.frame(predict(dummy, newdata = cleaned_data)) 
  # 
  # encoded <- fastDummies::dummy_cols(cleaned_data, remove_first_dummy = TRUE)
  # 
  # coupon_data_encoded <- encoded[ , ((!(colnames(encoded) %in% colnames(cleaned_data))) 
  #                                    | (colnames(encoded) %in% c("Y","age_weightage","income_weightage","expiration_weightage")))]
 

```
<!-- ## PCA -->

```{r}
# newdata_pca <- prcomp(coupon_data_encoded[,-(which(colnames(coupon_data_encoded)=="Y"))], center = TRUE,scale. = TRUE)
# summary(newdata_pca) # need to select PC32 for a cumulative variance of > 80
# newdata_pca$sdev
# View(newdata_pca$rotation)
```


```{r}
```

